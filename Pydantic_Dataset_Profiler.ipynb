{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renuka239/Deep-Learning-Project/blob/main/Pydantic_Dataset_Profiler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I built a dataset ingestion and profiling layer using Pydantic schemas. Any uploaded CSV is converted into a DatasetSchema capturing column metadata, inferred data types, missingness, and statistics, along with a DataPreview for downstream analytics, visualization, and LLM interaction."
      ],
      "metadata": {
        "id": "VWBbC6nQcubY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1: Install packages\n",
        "-q just means “quiet” (less output)."
      ],
      "metadata": {
        "id": "3TLtvOXsQ5RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas pydantic numpy\n"
      ],
      "metadata": {
        "id": "9w0RgLr-P1BL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: Imports\n",
        "typing → for type hints in Pydantic models\n",
        "datetime → timestamp when schema is created\n",
        "io → read uploaded file bytes as a file-like object\n",
        "pandas → read and manipulate CSV\n",
        "numpy → handle NaN and numeric conversions\n",
        "Pydantic → define schemas (your “key data structures”)\n",
        "Colab files → browser-based file upload"
      ],
      "metadata": {
        "id": "8n3MQztpRHsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from typing import Any, Dict, List, Optional, Literal\n",
        "from datetime import datetime\n",
        "import io\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pydantic import BaseModel, Field, ConfigDict\n",
        "\n",
        "from google.colab import files\n"
      ],
      "metadata": {
        "id": "hGbMTu-6QLQV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: Pydantic Schemas (KEY DATA STRUCTURES)\n",
        "1)DataType-This restricts column types to known values.\n",
        "So no random strings like \"int64\" or \"object\"\n",
        "2)ColumnSchema-Represents ONE column in the dataset.Each column stores:\n",
        "name → column name\n",
        "dtype → inferred logical type\n",
        "nullable → does it contain missing values?\n",
        "unique → does it look like an ID?\n",
        "Statistics:non-null count\n",
        "null count + percentage\n",
        "distinct values\n",
        "min / max / mean (only for numeric columns)\n",
        "This is your column-level metadata.\n",
        "\n",
        "3)DatasetSchema-Represents the ENTIRE dataset.It stores:dataset name (uploaded filename),timestamp (when schema was created),number of rows,number of column,list of ColumnSchema\n",
        "Think of this as a data catalog entry.\n",
        "\n",
        "4)DataPreview-A safe sample of the dataset.Why it exists:You don’t want to send full data to UI/LLMs. You want just a few rows, JSON-safe.It stores:dataset name,sample size, first few rows as dictionaries"
      ],
      "metadata": {
        "id": "ysMJejVrRatV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DataType = Literal[\"string\", \"integer\", \"number\", \"boolean\", \"datetime\", \"category\", \"unknown\"]\n",
        "\n",
        "class ColumnSchema(BaseModel):\n",
        "    model_config = ConfigDict(extra=\"forbid\")\n",
        "\n",
        "    name: str\n",
        "    dtype: DataType\n",
        "    nullable: bool\n",
        "    unique: bool\n",
        "\n",
        "    non_null_count: int\n",
        "    null_count: int\n",
        "    null_pct: float\n",
        "\n",
        "    distinct_count: int\n",
        "    min: Optional[float] = None\n",
        "    max: Optional[float] = None\n",
        "    mean: Optional[float] = None\n",
        "\n",
        "\n",
        "class DatasetSchema(BaseModel):\n",
        "    model_config = ConfigDict(extra=\"forbid\")\n",
        "\n",
        "    dataset_name: str\n",
        "    created_at: datetime = Field(default_factory=datetime.utcnow)\n",
        "\n",
        "    row_count: int\n",
        "    column_count: int\n",
        "    columns: List[ColumnSchema]\n",
        "\n",
        "\n",
        "class DataPreview(BaseModel):\n",
        "    model_config = ConfigDict(extra=\"forbid\")\n",
        "\n",
        "    dataset_name: str\n",
        "    sample_size: int\n",
        "    rows: List[Dict[str, Any]]\n"
      ],
      "metadata": {
        "id": "A1g9grKgQOEz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: Helper function (data type inference)This function decides:\n",
        "\n",
        "“What kind of data is this column logically?”\n",
        "\n",
        "Checks happen in order:Integer → \"integer\",Float → \"number\",Boolean → \"boolean\",Datetime → \"datetime\",Low-cardinality → \"category\",Otherwise → \"string\"\n",
        "This makes the code dataset-agnostic."
      ],
      "metadata": {
        "id": "vLKmwdkZbHWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_dtype(series: pd.Series) -> DataType:\n",
        "    if pd.api.types.is_integer_dtype(series):\n",
        "        return \"integer\"\n",
        "    if pd.api.types.is_float_dtype(series):\n",
        "        return \"number\"\n",
        "    if pd.api.types.is_bool_dtype(series):\n",
        "        return \"boolean\"\n",
        "    if pd.api.types.is_datetime64_any_dtype(series):\n",
        "        return \"datetime\"\n",
        "    if series.nunique(dropna=True) < 20:\n",
        "        return \"category\"\n",
        "    return \"string\"\n"
      ],
      "metadata": {
        "id": "KGEwIPe1QPeC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5: Upload CSV (Colab browser upload)-This opens a browser upload dialog.\n",
        "What’s happening:It gives you the file as bytes,io.BytesIO converts bytes → file-like object,Pandas reads it like a normal CSV\n",
        "Now df is a DataFrame."
      ],
      "metadata": {
        "id": "I9iSBercbbTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()  # choose a CSV file in the browser\n",
        "\n",
        "if len(uploaded) == 0:\n",
        "    raise ValueError(\"No file uploaded. Please upload a CSV file.\")\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "\n",
        "print(\"Loaded file:\", filename)\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "yJy3cj7RQT6L",
        "outputId": "e0ec4973-7466-4023-c559-aa6da1b97a06"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d223e4e8-e4f2-4c7c-bd83-98ad4c9c908e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d223e4e8-e4f2-4c7c-bd83-98ad4c9c908e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving OnlineRetail.csv to OnlineRetail (1).csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0xa3 in position 79780: invalid start byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2036485973.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded file:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa3 in position 79780: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6: Build DatasetSchema (core logic)-This is where raw data → structured schema happens.For each column:Infer data type,Count missing values,Compute missing percentage,Count distinct values,Check uniqueness\n",
        "If numeric → compute min, max, mean\n",
        "All these values are passed into ColumnSchema.\n",
        "Each column schema is appended to a list.\n",
        "Create DatasetSchema-This bundles:filename,row count,column count,all column schemas\n",
        "Now you have one validated object representing the dataset."
      ],
      "metadata": {
        "id": "sLk0S1b3btlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = []\n",
        "rows = len(df)\n",
        "\n",
        "for col in df.columns:\n",
        "    s = df[col]\n",
        "    dtype = infer_dtype(s)\n",
        "\n",
        "    null_count = int(s.isna().sum())\n",
        "    non_null_count = rows - null_count\n",
        "    null_pct = round((null_count / rows) * 100, 2)\n",
        "\n",
        "    distinct_count = int(s.nunique(dropna=True))\n",
        "    unique = distinct_count == non_null_count\n",
        "\n",
        "    min_v = max_v = mean_v = None\n",
        "    if dtype in [\"integer\", \"number\"]:\n",
        "        # numeric stats (safe conversion)\n",
        "        numeric = pd.to_numeric(s, errors=\"coerce\")\n",
        "        if numeric.notna().any():\n",
        "            min_v = float(numeric.min())\n",
        "            max_v = float(numeric.max())\n",
        "            mean_v = float(numeric.mean())\n",
        "\n",
        "    columns.append(\n",
        "        ColumnSchema(\n",
        "            name=str(col),\n",
        "            dtype=dtype,\n",
        "            nullable=null_count > 0,\n",
        "            unique=bool(unique),\n",
        "            non_null_count=non_null_count,\n",
        "            null_count=null_count,\n",
        "            null_pct=null_pct,\n",
        "            distinct_count=distinct_count,\n",
        "            min=min_v,\n",
        "            max=max_v,\n",
        "            mean=mean_v,\n",
        "        )\n",
        "    )\n",
        "\n",
        "dataset_schema = DatasetSchema(\n",
        "    dataset_name=filename,\n",
        "    row_count=rows,\n",
        "    column_count=len(df.columns),\n",
        "    columns=columns\n",
        ")\n",
        "\n",
        "dataset_schema\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncUHQEE6QY9t",
        "outputId": "d10d2351-2c4d-4d70-8a21-c234ca2aa576"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetSchema(dataset_name='Superstore Sales Dataset-train.csv', created_at=datetime.datetime(2026, 2, 7, 0, 16, 34, 365875), row_count=9800, column_count=18, columns=[ColumnSchema(name='Row ID', dtype='integer', nullable=False, unique=True, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=9800, min=1.0, max=9800.0, mean=4900.5), ColumnSchema(name='Order ID', dtype='string', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=4922, min=None, max=None, mean=None), ColumnSchema(name='Order Date', dtype='string', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=1230, min=None, max=None, mean=None), ColumnSchema(name='Ship Date', dtype='string', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=1326, min=None, max=None, mean=None), ColumnSchema(name='Ship Mode', dtype='category', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=4, min=None, max=None, mean=None), ColumnSchema(name='Customer ID', dtype='string', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=793, min=None, max=None, mean=None), ColumnSchema(name='Customer Name', dtype='string', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=793, min=None, max=None, mean=None), ColumnSchema(name='Segment', dtype='category', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=3, min=None, max=None, mean=None), ColumnSchema(name='Country', dtype='category', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=1, min=None, max=None, mean=None), ColumnSchema(name='City', dtype='string', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=529, min=None, max=None, mean=None), ColumnSchema(name='State', dtype='string', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=49, min=None, max=None, mean=None), ColumnSchema(name='Postal Code', dtype='number', nullable=True, unique=False, non_null_count=9789, null_count=11, null_pct=0.11, distinct_count=626, min=1040.0, max=99301.0, mean=55273.322402696904), ColumnSchema(name='Region', dtype='category', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=4, min=None, max=None, mean=None), ColumnSchema(name='Product ID', dtype='string', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=1861, min=None, max=None, mean=None), ColumnSchema(name='Category', dtype='category', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=3, min=None, max=None, mean=None), ColumnSchema(name='Sub-Category', dtype='category', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=17, min=None, max=None, mean=None), ColumnSchema(name='Product Name', dtype='string', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=1849, min=None, max=None, mean=None), ColumnSchema(name='Sales', dtype='number', nullable=False, unique=False, non_null_count=9800, null_count=0, null_pct=0.0, distinct_count=5757, min=0.444, max=22638.48, mean=230.7690594591837)])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7: Create DataPreview-Key things here:Takes first 5 rows,Converts NaN → None (JSON-safe),Converts rows into list of dictionaries\n",
        "This is perfect for:UI display,LLM prompts,Debugging,Logs"
      ],
      "metadata": {
        "id": "OGtbd_cscT27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preview = DataPreview(\n",
        "    dataset_name=filename,\n",
        "    sample_size=5,\n",
        "    rows=df.head(5).replace({np.nan: None}).to_dict(orient=\"records\"),\n",
        ")\n",
        "\n",
        "preview\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLyRZrhaQaun",
        "outputId": "e9e2157e-6fbf-4ba1-d743-faaded7d5c32"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataPreview(dataset_name='Superstore Sales Dataset-train.csv', sample_size=5, rows=[{'Row ID': 1, 'Order ID': 'CA-2017-152156', 'Order Date': '08/11/2017', 'Ship Date': '11/11/2017', 'Ship Mode': 'Second Class', 'Customer ID': 'CG-12520', 'Customer Name': 'Claire Gute', 'Segment': 'Consumer', 'Country': 'United States', 'City': 'Henderson', 'State': 'Kentucky', 'Postal Code': 42420.0, 'Region': 'South', 'Product ID': 'FUR-BO-10001798', 'Category': 'Furniture', 'Sub-Category': 'Bookcases', 'Product Name': 'Bush Somerset Collection Bookcase', 'Sales': 261.96}, {'Row ID': 2, 'Order ID': 'CA-2017-152156', 'Order Date': '08/11/2017', 'Ship Date': '11/11/2017', 'Ship Mode': 'Second Class', 'Customer ID': 'CG-12520', 'Customer Name': 'Claire Gute', 'Segment': 'Consumer', 'Country': 'United States', 'City': 'Henderson', 'State': 'Kentucky', 'Postal Code': 42420.0, 'Region': 'South', 'Product ID': 'FUR-CH-10000454', 'Category': 'Furniture', 'Sub-Category': 'Chairs', 'Product Name': 'Hon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back', 'Sales': 731.94}, {'Row ID': 3, 'Order ID': 'CA-2017-138688', 'Order Date': '12/06/2017', 'Ship Date': '16/06/2017', 'Ship Mode': 'Second Class', 'Customer ID': 'DV-13045', 'Customer Name': 'Darrin Van Huff', 'Segment': 'Corporate', 'Country': 'United States', 'City': 'Los Angeles', 'State': 'California', 'Postal Code': 90036.0, 'Region': 'West', 'Product ID': 'OFF-LA-10000240', 'Category': 'Office Supplies', 'Sub-Category': 'Labels', 'Product Name': 'Self-Adhesive Address Labels for Typewriters by Universal', 'Sales': 14.62}, {'Row ID': 4, 'Order ID': 'US-2016-108966', 'Order Date': '11/10/2016', 'Ship Date': '18/10/2016', 'Ship Mode': 'Standard Class', 'Customer ID': 'SO-20335', 'Customer Name': \"Sean O'Donnell\", 'Segment': 'Consumer', 'Country': 'United States', 'City': 'Fort Lauderdale', 'State': 'Florida', 'Postal Code': 33311.0, 'Region': 'South', 'Product ID': 'FUR-TA-10000577', 'Category': 'Furniture', 'Sub-Category': 'Tables', 'Product Name': 'Bretford CR4500 Series Slim Rectangular Table', 'Sales': 957.5775}, {'Row ID': 5, 'Order ID': 'US-2016-108966', 'Order Date': '11/10/2016', 'Ship Date': '18/10/2016', 'Ship Mode': 'Standard Class', 'Customer ID': 'SO-20335', 'Customer Name': \"Sean O'Donnell\", 'Segment': 'Consumer', 'Country': 'United States', 'City': 'Fort Lauderdale', 'State': 'Florida', 'Postal Code': 33311.0, 'Region': 'South', 'Product ID': 'OFF-ST-10000760', 'Category': 'Office Supplies', 'Sub-Category': 'Storage', 'Product Name': \"Eldon Fold 'N Roll Cart System\", 'Sales': 22.368}])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}